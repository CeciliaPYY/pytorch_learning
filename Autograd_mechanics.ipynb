# This is an record of Autograd Learning
# 每一个变量都有两种flag，分别是requires_grad和volatile。只有当所有输入的时候没有requires_grad参数时，输出才不需要require_grad参数。
# 一旦有一个输入包括requires_grad参数，输出就包括requries_grad参数。

import torch
from torch.autograd import Variable
x = Variable(torch.randn(5,5))
y = Variable(torch.randn(5,5))
a = x + y
a.requires_grad
z = Variable(torch.randn(5,5), requires_grad=True)
b = x + z 
b.requires_grad

# requires_grad的主要作用是，当我们需要固定模型的前几层。比如你想要finetune一个预先训练好了的CNN，对于前面的参数可以设置requires_grad
=False

import torchvision
model = torchvision.models.resnet18(pretrained=True)
for param in model.parameters():
    param.requires_grad = False
model.fc = torch.nn.Linear(512,100)
optimizer = torch.optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9)

# volatile = True 等价于 requires_grad = False,而且只要有一个volatile输入，那么输出一定是volatile 

regular_input = Variable(torch.randn(1,3,227,227))
volatile_input = Variable(torch.randn(1,3,227,227), volatile=True)
model = torchvision.models.resnet18(pretrained=True)
model(regular_input).requires_grad #True
model(volatile_input).requires_grad #False
model(volatile_input).volatile #True
model(volatile_input).grad_fn is None #True
